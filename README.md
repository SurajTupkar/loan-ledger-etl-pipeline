ğŸ“Œ Loan Ledger ETL Pipeline : 

Overview

The Loan Ledger ETL Pipeline is an end-to-end data engineering project built using PySpark to process, validate, transform, and load loan-related financial data.

This pipeline is designed to simulate a real-world production data system, supporting both local and cloud (AWS S3) environments using a configuration-driven architecture.

Features

```
âœ… Config-driven environment setup (Local / AWS)
âœ… Scalable data ingestion from file system / S3
âœ… Data cleaning and validation
âœ… Business rule transformations
âœ… Data quality checks
âœ… Curated data storage
âœ… Modular load layer (S3 / RDS ready)
âœ… Centralized logging
âœ… Test-ready structure
```

Project Architecture :

```
Extract  â†’  Transform  â†’  Quality Check  â†’  Load
   â†“           â†“             â†“              â†“
 Local/S3   Cleaning     Validation      S3 / RDS

```

ğŸ“‚ Folder Structure

```

src/
â”œâ”€â”€ main.py
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ etl_job.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ app_config.yaml
â”œâ”€â”€ extract/
â”‚   â””â”€â”€ s3_reader.py
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â””â”€â”€ business_rules.py
â”œâ”€â”€ quality/
â”‚   â””â”€â”€ data_quality.py
â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ s3_writer.py
â”‚   â””â”€â”€ rds_writer.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ spark_session.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ common.py
tests/
deployment/
requirements.txt
README.md

```

âš™ï¸ Technology Stack

Language: Python
Processing Engine: PySpark
Storage: Local / AWS S3
Database: MySQL (RDS â€“ optional)
Configuration: YAML
Logging: Python Logging
Version Control: Git & GitHub


ğŸ”§ Configuration

All environment-specific configurations are maintained in:
config/app_config.yaml

Example:

environment: local

paths:
  local:
    input: /mnt/d/data/loan.csv
    output: /mnt/d/output/

  aws:
    input: s3://bucket/raw/
    output: s3://bucket/curated/

Change the environment to switch between local and AWS.


â–¶ï¸ How to Run the Pipeline

```

1ï¸âƒ£ Create Virtual Environment
python3 -m venv venv
source venv/bin/activate
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
3ï¸âƒ£ Run the ETL Job
spark-submit main.py

```


ğŸ§ª Testing
Unit tests are available under the tests/ directory.
pytest tests/


ğŸ“Š Logging
Logs are generated for every run in:
logs/


Example format:
2026-02-14 11:23:35 | INFO | ETL-Job | Job started


ğŸ—„ï¸ Data Flow

1. Extract data from Local / S3
2. Clean and standardize records
3. Apply business rules
4. Perform data quality checks
5. Store curated output
6. (Optional) Load into RDS


ğŸ› ï¸ Current Status

```
-------------------------------------------------
Module	                     Status             
Local Processing	            âœ…                 
AWS S3 Support	               âœ…                 
Data Quality	               âœ…
Logging	                     âœ…
RDS Integration	            â³ In Progress
Incremental Load	            â³ Planned
Airflow Orchestration   	    â³ Planned
-------------------------------------------------
```

ğŸ“ˆ Future Enhancements

ğŸ”¹ Incremental load using watermarking
ğŸ”¹ Schema versioning
ğŸ”¹ Idempotent writes
ğŸ”¹ Audit & reconciliation framework
ğŸ”¹ Airflow DAG orchestration
ğŸ”¹ Retry & failure handling
ğŸ”¹ Performance optimization


ğŸ’¼ Use Case

This project simulates how financial institutions process loan ledger data for:

Reporting
Compliance
Analytics
Risk assessment


ğŸ‘¨â€ğŸ’» Author

Suraj Tupkar
Data Engineer 
Python | SQL | PySpark | AWS | ETL Pipelines





